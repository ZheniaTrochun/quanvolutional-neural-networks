{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DzWy2QT7t3n","outputId":"f8b652a3-1e6a-498b-cd9a-d9a8b060627c","executionInfo":{"status":"ok","timestamp":1708130277880,"user_tz":-120,"elapsed":31236,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu121)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}],"source":["# !pip install torch==2.2\n","!pip install torch\n","!pip install torchvision\n","!pip install seaborn\n","!pip install numpy\n","!pip install matplotlib\n","# !pip install torchsummary\n","# !pip install torchview\n","# !pip install graphviz\n","# !pip install torchviz\n","!pip install pandas\n","# !pip install efficientnet_pytorch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aIAEtl027t3p","executionInfo":{"status":"ok","timestamp":1708130284765,"user_tz":-120,"elapsed":6887,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset, DataLoader, Subset\n","from torch.utils.data import random_split\n","import seaborn as sns\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ImXF4sXm8GY0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130321417,"user_tz":-120,"elapsed":36655,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"bf00c0e8-c775-4704-8c8d-02f56d0096e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mvWm7omZ7t3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130321417,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"269b66c7-9b7d-47f8-f07a-f9c2fc6d0c86"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_D0laDM27t3p","executionInfo":{"status":"ok","timestamp":1708130321418,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["BATCH_SIZE = 64\n","num_epoch = 10\n","learning_rate = 1e-4\n","class_size = 10"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_B9EZyw27t3p","executionInfo":{"status":"ok","timestamp":1708130321418,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# transform_train = transforms.Compose([transforms.Resize((224, 224)),\n","#                                       transforms.RandomHorizontalFlip(p=0.7),\n","#                                       transforms.ToTensor(),\n","#                                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","# transform_test = transforms.Compose([transforms.Resize((224, 224)),\n","#                                      transforms.ToTensor(),\n","#                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uRpwm0-lwTcJ","executionInfo":{"status":"ok","timestamp":1708130321418,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["transform_train = transforms.Compose([transforms.Resize((224, 224)),\n","                                      transforms.RandomHorizontalFlip(p=0.7)])\n","\n","transform_test = transforms.Compose([transforms.Resize((224, 224))])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"MHGjZijytuHu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130321865,"user_tz":-120,"elapsed":449,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"97a3597d-8a3b-4b8b-fead-5eaaa1808a5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["QNN_TRAIN_CPU_hurricane_W4_X_S0_E10000_s1.npy\t  QNN_TRAIN_CPU_hurricane_W4_XY_S0_E5000_s1.npy\n","QNN_TRAIN_CPU_hurricane_W4_X_S0_E10000_s2.npy\t  QNN_TRAIN_CPU_hurricane_W4_XY_S5000_E10000_s1.npy\n","QNN_TRAIN_CPU_hurricane_W4_X_S0_E5000_s1.npy\t  raw_hurricane_dataset\n","QNN_TRAIN_CPU_hurricane_W4_X_S5000_E10000_s1.npy  train_another_labels.npy\n","QNN_TRAIN_CPU_hurricane_W4_XY_S0_E10000_s1.npy\n"]}],"source":["!ls /content/drive/MyDrive/hurricane_damage/"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"8p_JL-OuKW9C","executionInfo":{"status":"ok","timestamp":1708130423996,"user_tz":-120,"elapsed":102133,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["q_train_images = np.load(\"/content/drive/MyDrive/hurricane_damage/QNN_TRAIN_CPU_hurricane_W4_XY_S0_E10000_s1.npy\")\n","\n","# q_train_images_1 = np.load(\"/content/drive/MyDrive/hurricane_damage/QNN_TRAIN_CPU_hurricane_W4_XY_S0_E5000_s1.npy\")\n","# q_train_images_2 = np.load(\"/content/drive/MyDrive/hurricane_damage/QNN_TRAIN_CPU_hurricane_W4_XY_S5000_E10000_s1.npy\")\n","# q_train_images = np.concatenate((q_train_images_1, q_train_images_2), axis=0)\n","# np.save(\"/content/drive/MyDrive/hurricane_damage/QNN_TRAIN_CPU_hurricane_W4_XY_S0_E10000_s1.npy\", q_train_images)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oi4WFny8PMIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130435660,"user_tz":-120,"elapsed":11674,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"a8e91d9a-34ba-4dd2-a813-baec5d1b4040"},"outputs":[{"output_type":"stream","name":"stdout","text":["shape of raw quantum train data: (10000, 127, 127, 3, 4)\n","shape of quantum train data: (40000, 3, 127, 127)\n"]}],"source":["print(f'shape of raw quantum train data: {q_train_images.shape}')\n","train_output_shape = (4 * 10000, 127, 127, 3)\n","q_train_images = np.transpose(q_train_images, (4, 0, 1, 2, 3))\n","q_train_images = np.reshape(q_train_images, train_output_shape)\n","q_train_images = np.transpose(q_train_images, (0, 3, 1, 2))\n","print(f'shape of quantum train data: {q_train_images.shape}')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"FiUuQoX27t3p","executionInfo":{"status":"ok","timestamp":1708130463003,"user_tz":-120,"elapsed":27351,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["torch.manual_seed(2024)\n","\n","train_ref = torchvision.datasets.ImageFolder(\"/content/drive/MyDrive/hurricane_damage/raw_hurricane_dataset/train_another/\", transform=transforms.ToTensor())\n","# train_ref_loader = DataLoader(train_ref, batch_size=1)\n","\n","# labels = []\n","# for _, (_, label) in enumerate(train_ref_loader):\n","#   labels.append(label.item())\n","\n","# print(len(labels))\n","\n","# val_size = 2000\n","# train_size = len(train) - val_size\n","\n","# train, val = random_split(train, [train_size, val_size])\n","\n","train_loader_ref = DataLoader(train_ref, batch_size=BATCH_SIZE, shuffle=False)\n","# test_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ShpaIvJXZmwq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130463463,"user_tz":-120,"elapsed":469,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"d275e498-ffa4-49d7-d159-c6247cafd4a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000,)\n"]}],"source":["labels = np.load(\"/content/drive/MyDrive/hurricane_damage/train_another_labels.npy\")\n","print(labels.shape)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"dkxcnIE2Z-PN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130463463,"user_tz":-120,"elapsed":2,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"df932f7f-1e44-4814-bc4b-9791c1deeeee"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([40000])\n"]}],"source":["transformed_labels = torch.Tensor(labels.tolist() * 4).to(torch.int64)\n","print(transformed_labels.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6ob4ZFCaakTg","executionInfo":{"status":"ok","timestamp":1708130465148,"user_tz":-120,"elapsed":1686,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["train_dataset = TensorDataset(torch.Tensor(q_train_images), transformed_labels)\n","\n","val_size = 8000\n","\n","train_size = len(train_dataset) - val_size\n","\n","train, val = random_split(train_dataset, [train_size, val_size])\n","train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from torchvision.utils import make_grid\n","\n","for images, labels in train_loader:\n","    print('shape: ', images.shape)\n","    print(labels.tolist())\n","    plt.figure(figsize=(100, 100))\n","    plt.axis('off')\n","    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n","    break"],"metadata":{"id":"RPtLRVK4GPUj","colab":{"base_uri":"https://localhost:8080/","height":360,"output_embedded_package_id":"12XsbJphtU24O_J77HPJZe-0xAqfy3pN3"},"executionInfo":{"status":"ok","timestamp":1708130475104,"user_tz":-120,"elapsed":9958,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"fe488e49-fcec-4b48-c1b9-bad05e86016a"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"AXq4wPKi7t3q","executionInfo":{"status":"ok","timestamp":1708130475104,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# from torchvision.utils import make_grid\n","\n","# sub = Subset(train_dataset, [32000 + x for x in range(1000)])\n","# # 33000 - 35100, 38000 - 40000\n","# # 13000 - 15000, 18000 - 20000 ??\n","# # 20000 - 23000, 28000 - 29000 ???\n","# # for images, labels in train_loader:\n","# for images, labels in DataLoader(sub, batch_size=1000, shuffle=False):\n","#     print('shape: ', images.shape)\n","#     print(labels.tolist())\n","#     plt.figure(figsize=(100, 100))\n","#     plt.axis('off')\n","#     # plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n","#     plt.imshow(make_grid(images, nrow=10).permute((1, 2, 0)))\n","#     break"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vgOdVbOgteQC","executionInfo":{"status":"ok","timestamp":1708130475104,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# from torchvision.utils import make_grid\n","\n","# sub_ref = Subset(train_ref, [4990 + x for x in range(64)])\n","\n","# for images, labels in DataLoader(sub_ref, batch_size=BATCH_SIZE, shuffle=False):\n","#     print('shape: ', images.shape)\n","#     print(labels.tolist())\n","#     plt.figure(figsize=(16, 8))\n","#     plt.axis('off')\n","#     # plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n","#     plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n","#     break"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"wsBYgxGc7t3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130475104,"user_tz":-120,"elapsed":12,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"fcb6888b-b52f-4cc7-810b-42ec4c3061fc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"]}],"source":["from torchvision import models\n","\n","model = models.resnet18(pretrained=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"TYGwPH7c7t3q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130475105,"user_tz":-120,"elapsed":11,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"8111b409-fbf1-4b86-8664-b2ca04e1cd50"},"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"glNxZsRv7t3q","executionInfo":{"status":"ok","timestamp":1708130475105,"user_tz":-120,"elapsed":10,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# from torchsummary import summary\n","\n","# summary(model.cuda(), (3, 224, 224))"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"dIMSIL4r7t3r","executionInfo":{"status":"ok","timestamp":1708130475105,"user_tz":-120,"elapsed":10,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# from torchview import draw_graph\n","\n","# model_graph = draw_graph(model, input_size=(1, 3, 224, 224), expand_nested=True)\n","# model_graph.visual_graph"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"H28eERwC7t3r","executionInfo":{"status":"ok","timestamp":1708130475653,"user_tz":-120,"elapsed":557,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["input_last_year = model.fc.in_features\n","model.fc = nn.Linear(input_last_year, 2)\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"RLZih5fa7t3r","executionInfo":{"status":"ok","timestamp":1708130475653,"user_tz":-120,"elapsed":4,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["epoch_loss_history = []\n","epoch_acc_history = []"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"EnnQpTMx7t3r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"00ceeb4a-78ce-4217-a4f7-d03a99e5afab","executionInfo":{"status":"ok","timestamp":1708130802038,"user_tz":-120,"elapsed":326388,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1/10, step: 10/500.0: loss = 0.22910, acc = 90.62\n","epoch 1/10, step: 20/500.0: loss = 0.11946, acc = 93.75\n","epoch 1/10, step: 30/500.0: loss = 0.20753, acc = 93.75\n","epoch 1/10, step: 40/500.0: loss = 0.12519, acc = 93.75\n","epoch 1/10, step: 50/500.0: loss = 0.13165, acc = 96.88\n","epoch 1/10, step: 60/500.0: loss = 0.13245, acc = 92.19\n","epoch 1/10, step: 70/500.0: loss = 0.06093, acc = 98.44\n","epoch 1/10, step: 80/500.0: loss = 0.12126, acc = 93.75\n","epoch 1/10, step: 90/500.0: loss = 0.05343, acc = 98.44\n","epoch 1/10, step: 100/500.0: loss = 0.01358, acc = 100.00\n","epoch 1/10, step: 110/500.0: loss = 0.04737, acc = 98.44\n","epoch 1/10, step: 120/500.0: loss = 0.02813, acc = 100.00\n","epoch 1/10, step: 130/500.0: loss = 0.07041, acc = 96.88\n","epoch 1/10, step: 140/500.0: loss = 0.11099, acc = 93.75\n","epoch 1/10, step: 150/500.0: loss = 0.08813, acc = 96.88\n","epoch 1/10, step: 160/500.0: loss = 0.08213, acc = 98.44\n","epoch 1/10, step: 170/500.0: loss = 0.04604, acc = 100.00\n","epoch 1/10, step: 180/500.0: loss = 0.09854, acc = 96.88\n","epoch 1/10, step: 190/500.0: loss = 0.04849, acc = 98.44\n","epoch 1/10, step: 200/500.0: loss = 0.12414, acc = 95.31\n","epoch 1/10, step: 210/500.0: loss = 0.09082, acc = 96.88\n","epoch 1/10, step: 220/500.0: loss = 0.06812, acc = 95.31\n","epoch 1/10, step: 230/500.0: loss = 0.01898, acc = 100.00\n","epoch 1/10, step: 240/500.0: loss = 0.01087, acc = 100.00\n","epoch 1/10, step: 250/500.0: loss = 0.08616, acc = 96.88\n","epoch 1/10, step: 260/500.0: loss = 0.02946, acc = 100.00\n","epoch 1/10, step: 270/500.0: loss = 0.00660, acc = 100.00\n","epoch 1/10, step: 280/500.0: loss = 0.03642, acc = 98.44\n","epoch 1/10, step: 290/500.0: loss = 0.01470, acc = 100.00\n","epoch 1/10, step: 300/500.0: loss = 0.07374, acc = 96.88\n","epoch 1/10, step: 310/500.0: loss = 0.09043, acc = 96.88\n","epoch 1/10, step: 320/500.0: loss = 0.04720, acc = 98.44\n","epoch 1/10, step: 330/500.0: loss = 0.02840, acc = 98.44\n","epoch 1/10, step: 340/500.0: loss = 0.04815, acc = 96.88\n","epoch 1/10, step: 350/500.0: loss = 0.01291, acc = 100.00\n","epoch 1/10, step: 360/500.0: loss = 0.04738, acc = 98.44\n","epoch 1/10, step: 370/500.0: loss = 0.01024, acc = 100.00\n","epoch 1/10, step: 380/500.0: loss = 0.11814, acc = 95.31\n","epoch 1/10, step: 390/500.0: loss = 0.07442, acc = 96.88\n","epoch 1/10, step: 400/500.0: loss = 0.02842, acc = 98.44\n","epoch 1/10, step: 410/500.0: loss = 0.03071, acc = 98.44\n","epoch 1/10, step: 420/500.0: loss = 0.04284, acc = 98.44\n","epoch 1/10, step: 430/500.0: loss = 0.02350, acc = 98.44\n","epoch 1/10, step: 440/500.0: loss = 0.03179, acc = 98.44\n","epoch 1/10, step: 450/500.0: loss = 0.06419, acc = 98.44\n","epoch 1/10, step: 460/500.0: loss = 0.02068, acc = 98.44\n","epoch 1/10, step: 470/500.0: loss = 0.01435, acc = 100.00\n","epoch 1/10, step: 480/500.0: loss = 0.00412, acc = 100.00\n","epoch 1/10, step: 490/500.0: loss = 0.00551, acc = 100.00\n","epoch 1/10, step: 500/500.0: loss = 0.01905, acc = 98.44\n","epoch 1/10, val accuracy = 98.46%. Correct 7877 out of 8000 samples\n","epoch 2/10, step: 10/500.0: loss = 0.00896, acc = 100.00\n","epoch 2/10, step: 20/500.0: loss = 0.00272, acc = 100.00\n","epoch 2/10, step: 30/500.0: loss = 0.00363, acc = 100.00\n","epoch 2/10, step: 40/500.0: loss = 0.03029, acc = 96.88\n","epoch 2/10, step: 50/500.0: loss = 0.10288, acc = 96.88\n","epoch 2/10, step: 60/500.0: loss = 0.02880, acc = 96.88\n","epoch 2/10, step: 70/500.0: loss = 0.00181, acc = 100.00\n","epoch 2/10, step: 80/500.0: loss = 0.02206, acc = 100.00\n","epoch 2/10, step: 90/500.0: loss = 0.01774, acc = 100.00\n","epoch 2/10, step: 100/500.0: loss = 0.00624, acc = 100.00\n","epoch 2/10, step: 110/500.0: loss = 0.00246, acc = 100.00\n","epoch 2/10, step: 120/500.0: loss = 0.00313, acc = 100.00\n","epoch 2/10, step: 130/500.0: loss = 0.00127, acc = 100.00\n","epoch 2/10, step: 140/500.0: loss = 0.03107, acc = 98.44\n","epoch 2/10, step: 150/500.0: loss = 0.00664, acc = 100.00\n","epoch 2/10, step: 160/500.0: loss = 0.00316, acc = 100.00\n","epoch 2/10, step: 170/500.0: loss = 0.01388, acc = 100.00\n","epoch 2/10, step: 180/500.0: loss = 0.00203, acc = 100.00\n","epoch 2/10, step: 190/500.0: loss = 0.00563, acc = 100.00\n","epoch 2/10, step: 200/500.0: loss = 0.00558, acc = 100.00\n","epoch 2/10, step: 210/500.0: loss = 0.00255, acc = 100.00\n","epoch 2/10, step: 220/500.0: loss = 0.12287, acc = 96.88\n","epoch 2/10, step: 230/500.0: loss = 0.02019, acc = 98.44\n","epoch 2/10, step: 240/500.0: loss = 0.07105, acc = 98.44\n","epoch 2/10, step: 250/500.0: loss = 0.06548, acc = 96.88\n","epoch 2/10, step: 260/500.0: loss = 0.04274, acc = 98.44\n","epoch 2/10, step: 270/500.0: loss = 0.00608, acc = 100.00\n","epoch 2/10, step: 280/500.0: loss = 0.06978, acc = 98.44\n","epoch 2/10, step: 290/500.0: loss = 0.00304, acc = 100.00\n","epoch 2/10, step: 300/500.0: loss = 0.02020, acc = 100.00\n","epoch 2/10, step: 310/500.0: loss = 0.02449, acc = 98.44\n","epoch 2/10, step: 320/500.0: loss = 0.03457, acc = 98.44\n","epoch 2/10, step: 330/500.0: loss = 0.00116, acc = 100.00\n","epoch 2/10, step: 340/500.0: loss = 0.00196, acc = 100.00\n","epoch 2/10, step: 350/500.0: loss = 0.00084, acc = 100.00\n","epoch 2/10, step: 360/500.0: loss = 0.01481, acc = 98.44\n","epoch 2/10, step: 370/500.0: loss = 0.00165, acc = 100.00\n","epoch 2/10, step: 380/500.0: loss = 0.00726, acc = 100.00\n","epoch 2/10, step: 390/500.0: loss = 0.02080, acc = 98.44\n","epoch 2/10, step: 400/500.0: loss = 0.00883, acc = 100.00\n","epoch 2/10, step: 410/500.0: loss = 0.01392, acc = 98.44\n","epoch 2/10, step: 420/500.0: loss = 0.00709, acc = 100.00\n","epoch 2/10, step: 430/500.0: loss = 0.00130, acc = 100.00\n","epoch 2/10, step: 440/500.0: loss = 0.01020, acc = 100.00\n","epoch 2/10, step: 450/500.0: loss = 0.03639, acc = 98.44\n","epoch 2/10, step: 460/500.0: loss = 0.00127, acc = 100.00\n","epoch 2/10, step: 470/500.0: loss = 0.00201, acc = 100.00\n","epoch 2/10, step: 480/500.0: loss = 0.00234, acc = 100.00\n","epoch 2/10, step: 490/500.0: loss = 0.00073, acc = 100.00\n","epoch 2/10, step: 500/500.0: loss = 0.00345, acc = 100.00\n","epoch 2/10, val accuracy = 98.85%. Correct 7908 out of 8000 samples\n","epoch 3/10, step: 10/500.0: loss = 0.03619, acc = 98.44\n","epoch 3/10, step: 20/500.0: loss = 0.00993, acc = 100.00\n","epoch 3/10, step: 30/500.0: loss = 0.00846, acc = 100.00\n","epoch 3/10, step: 40/500.0: loss = 0.00250, acc = 100.00\n","epoch 3/10, step: 50/500.0: loss = 0.02455, acc = 98.44\n","epoch 3/10, step: 60/500.0: loss = 0.00898, acc = 100.00\n","epoch 3/10, step: 70/500.0: loss = 0.00393, acc = 100.00\n","epoch 3/10, step: 80/500.0: loss = 0.00983, acc = 100.00\n","epoch 3/10, step: 90/500.0: loss = 0.00096, acc = 100.00\n","epoch 3/10, step: 100/500.0: loss = 0.00092, acc = 100.00\n","epoch 3/10, step: 110/500.0: loss = 0.00271, acc = 100.00\n","epoch 3/10, step: 120/500.0: loss = 0.00084, acc = 100.00\n","epoch 3/10, step: 130/500.0: loss = 0.04325, acc = 98.44\n","epoch 3/10, step: 140/500.0: loss = 0.00519, acc = 100.00\n","epoch 3/10, step: 150/500.0: loss = 0.13615, acc = 95.31\n","epoch 3/10, step: 160/500.0: loss = 0.00575, acc = 100.00\n","epoch 3/10, step: 170/500.0: loss = 0.00073, acc = 100.00\n","epoch 3/10, step: 180/500.0: loss = 0.00552, acc = 100.00\n","epoch 3/10, step: 190/500.0: loss = 0.00814, acc = 100.00\n","epoch 3/10, step: 200/500.0: loss = 0.00437, acc = 100.00\n","epoch 3/10, step: 210/500.0: loss = 0.00155, acc = 100.00\n","epoch 3/10, step: 220/500.0: loss = 0.00604, acc = 100.00\n","epoch 3/10, step: 230/500.0: loss = 0.00161, acc = 100.00\n","epoch 3/10, step: 240/500.0: loss = 0.00122, acc = 100.00\n","epoch 3/10, step: 250/500.0: loss = 0.01458, acc = 100.00\n","epoch 3/10, step: 260/500.0: loss = 0.00107, acc = 100.00\n","epoch 3/10, step: 270/500.0: loss = 0.00104, acc = 100.00\n","epoch 3/10, step: 280/500.0: loss = 0.00571, acc = 100.00\n","epoch 3/10, step: 290/500.0: loss = 0.00042, acc = 100.00\n","epoch 3/10, step: 300/500.0: loss = 0.00551, acc = 100.00\n","epoch 3/10, step: 310/500.0: loss = 0.00704, acc = 100.00\n","epoch 3/10, step: 320/500.0: loss = 0.00113, acc = 100.00\n","epoch 3/10, step: 330/500.0: loss = 0.00348, acc = 100.00\n","epoch 3/10, step: 340/500.0: loss = 0.00025, acc = 100.00\n","epoch 3/10, step: 350/500.0: loss = 0.02094, acc = 98.44\n","epoch 3/10, step: 360/500.0: loss = 0.00386, acc = 100.00\n","epoch 3/10, step: 370/500.0: loss = 0.07307, acc = 98.44\n","epoch 3/10, step: 380/500.0: loss = 0.09558, acc = 95.31\n","epoch 3/10, step: 390/500.0: loss = 0.03660, acc = 98.44\n","epoch 3/10, step: 400/500.0: loss = 0.03201, acc = 98.44\n","epoch 3/10, step: 410/500.0: loss = 0.00253, acc = 100.00\n","epoch 3/10, step: 420/500.0: loss = 0.02832, acc = 98.44\n","epoch 3/10, step: 430/500.0: loss = 0.00714, acc = 100.00\n","epoch 3/10, step: 440/500.0: loss = 0.00105, acc = 100.00\n","epoch 3/10, step: 450/500.0: loss = 0.02360, acc = 98.44\n","epoch 3/10, step: 460/500.0: loss = 0.00057, acc = 100.00\n","epoch 3/10, step: 470/500.0: loss = 0.00575, acc = 100.00\n","epoch 3/10, step: 480/500.0: loss = 0.00082, acc = 100.00\n","epoch 3/10, step: 490/500.0: loss = 0.05509, acc = 98.44\n","epoch 3/10, step: 500/500.0: loss = 0.00129, acc = 100.00\n","epoch 3/10, val accuracy = 98.79%. Correct 7903 out of 8000 samples\n","epoch 4/10, step: 10/500.0: loss = 0.00579, acc = 100.00\n","epoch 4/10, step: 20/500.0: loss = 0.00027, acc = 100.00\n","epoch 4/10, step: 30/500.0: loss = 0.02035, acc = 98.44\n","epoch 4/10, step: 40/500.0: loss = 0.00384, acc = 100.00\n","epoch 4/10, step: 50/500.0: loss = 0.00129, acc = 100.00\n","epoch 4/10, step: 60/500.0: loss = 0.03837, acc = 98.44\n","epoch 4/10, step: 70/500.0: loss = 0.00050, acc = 100.00\n","epoch 4/10, step: 80/500.0: loss = 0.00519, acc = 100.00\n","epoch 4/10, step: 90/500.0: loss = 0.00098, acc = 100.00\n","epoch 4/10, step: 100/500.0: loss = 0.00326, acc = 100.00\n","epoch 4/10, step: 110/500.0: loss = 0.00101, acc = 100.00\n","epoch 4/10, step: 120/500.0: loss = 0.00067, acc = 100.00\n","epoch 4/10, step: 130/500.0: loss = 0.00276, acc = 100.00\n","epoch 4/10, step: 140/500.0: loss = 0.00613, acc = 100.00\n","epoch 4/10, step: 150/500.0: loss = 0.01028, acc = 100.00\n","epoch 4/10, step: 160/500.0: loss = 0.00196, acc = 100.00\n","epoch 4/10, step: 170/500.0: loss = 0.00324, acc = 100.00\n","epoch 4/10, step: 180/500.0: loss = 0.00146, acc = 100.00\n","epoch 4/10, step: 190/500.0: loss = 0.10747, acc = 98.44\n","epoch 4/10, step: 200/500.0: loss = 0.00102, acc = 100.00\n","epoch 4/10, step: 210/500.0: loss = 0.00044, acc = 100.00\n","epoch 4/10, step: 220/500.0: loss = 0.00225, acc = 100.00\n","epoch 4/10, step: 230/500.0: loss = 0.00013, acc = 100.00\n","epoch 4/10, step: 240/500.0: loss = 0.00078, acc = 100.00\n","epoch 4/10, step: 250/500.0: loss = 0.00105, acc = 100.00\n","epoch 4/10, step: 260/500.0: loss = 0.00060, acc = 100.00\n","epoch 4/10, step: 270/500.0: loss = 0.00037, acc = 100.00\n","epoch 4/10, step: 280/500.0: loss = 0.00121, acc = 100.00\n","epoch 4/10, step: 290/500.0: loss = 0.00230, acc = 100.00\n","epoch 4/10, step: 300/500.0: loss = 0.00045, acc = 100.00\n","epoch 4/10, step: 310/500.0: loss = 0.00003, acc = 100.00\n","epoch 4/10, step: 320/500.0: loss = 0.19088, acc = 96.88\n","epoch 4/10, step: 330/500.0: loss = 0.00028, acc = 100.00\n","epoch 4/10, step: 340/500.0: loss = 0.00037, acc = 100.00\n","epoch 4/10, step: 350/500.0: loss = 0.00007, acc = 100.00\n","epoch 4/10, step: 360/500.0: loss = 0.06026, acc = 98.44\n","epoch 4/10, step: 370/500.0: loss = 0.00260, acc = 100.00\n","epoch 4/10, step: 380/500.0: loss = 0.00019, acc = 100.00\n","epoch 4/10, step: 390/500.0: loss = 0.00076, acc = 100.00\n","epoch 4/10, step: 400/500.0: loss = 0.00069, acc = 100.00\n","epoch 4/10, step: 410/500.0: loss = 0.00225, acc = 100.00\n","epoch 4/10, step: 420/500.0: loss = 0.00377, acc = 100.00\n","epoch 4/10, step: 430/500.0: loss = 0.00019, acc = 100.00\n","epoch 4/10, step: 440/500.0: loss = 0.00009, acc = 100.00\n","epoch 4/10, step: 450/500.0: loss = 0.00013, acc = 100.00\n","epoch 4/10, step: 460/500.0: loss = 0.00016, acc = 100.00\n","epoch 4/10, step: 470/500.0: loss = 0.00580, acc = 100.00\n","epoch 4/10, step: 480/500.0: loss = 0.00056, acc = 100.00\n","epoch 4/10, step: 490/500.0: loss = 0.01472, acc = 98.44\n","epoch 4/10, step: 500/500.0: loss = 0.00346, acc = 100.00\n","epoch 4/10, val accuracy = 99.39%. Correct 7951 out of 8000 samples\n","epoch 5/10, step: 10/500.0: loss = 0.00112, acc = 100.00\n","epoch 5/10, step: 20/500.0: loss = 0.00007, acc = 100.00\n","epoch 5/10, step: 30/500.0: loss = 0.00443, acc = 100.00\n","epoch 5/10, step: 40/500.0: loss = 0.03645, acc = 96.88\n","epoch 5/10, step: 50/500.0: loss = 0.00159, acc = 100.00\n","epoch 5/10, step: 60/500.0: loss = 0.01416, acc = 100.00\n","epoch 5/10, step: 70/500.0: loss = 0.00024, acc = 100.00\n","epoch 5/10, step: 80/500.0: loss = 0.00256, acc = 100.00\n","epoch 5/10, step: 90/500.0: loss = 0.00080, acc = 100.00\n","epoch 5/10, step: 100/500.0: loss = 0.00042, acc = 100.00\n","epoch 5/10, step: 110/500.0: loss = 0.05281, acc = 98.44\n","epoch 5/10, step: 120/500.0: loss = 0.00023, acc = 100.00\n","epoch 5/10, step: 130/500.0: loss = 0.00026, acc = 100.00\n","epoch 5/10, step: 140/500.0: loss = 0.00978, acc = 100.00\n","epoch 5/10, step: 150/500.0: loss = 0.00501, acc = 100.00\n","epoch 5/10, step: 160/500.0: loss = 0.00247, acc = 100.00\n","epoch 5/10, step: 170/500.0: loss = 0.00086, acc = 100.00\n","epoch 5/10, step: 180/500.0: loss = 0.00588, acc = 100.00\n","epoch 5/10, step: 190/500.0: loss = 0.00709, acc = 100.00\n","epoch 5/10, step: 200/500.0: loss = 0.00295, acc = 100.00\n","epoch 5/10, step: 210/500.0: loss = 0.00519, acc = 100.00\n","epoch 5/10, step: 220/500.0: loss = 0.00459, acc = 100.00\n","epoch 5/10, step: 230/500.0: loss = 0.00171, acc = 100.00\n","epoch 5/10, step: 240/500.0: loss = 0.00089, acc = 100.00\n","epoch 5/10, step: 250/500.0: loss = 0.00651, acc = 100.00\n","epoch 5/10, step: 260/500.0: loss = 0.00565, acc = 100.00\n","epoch 5/10, step: 270/500.0: loss = 0.00036, acc = 100.00\n","epoch 5/10, step: 280/500.0: loss = 0.01013, acc = 100.00\n","epoch 5/10, step: 290/500.0: loss = 0.00074, acc = 100.00\n","epoch 5/10, step: 300/500.0: loss = 0.00922, acc = 100.00\n","epoch 5/10, step: 310/500.0: loss = 0.00059, acc = 100.00\n","epoch 5/10, step: 320/500.0: loss = 0.00099, acc = 100.00\n","epoch 5/10, step: 330/500.0: loss = 0.00028, acc = 100.00\n","epoch 5/10, step: 340/500.0: loss = 0.00055, acc = 100.00\n","epoch 5/10, step: 350/500.0: loss = 0.00041, acc = 100.00\n","epoch 5/10, step: 360/500.0: loss = 0.00066, acc = 100.00\n","epoch 5/10, step: 370/500.0: loss = 0.00564, acc = 100.00\n","epoch 5/10, step: 380/500.0: loss = 0.00099, acc = 100.00\n","epoch 5/10, step: 390/500.0: loss = 0.00040, acc = 100.00\n","epoch 5/10, step: 400/500.0: loss = 0.00070, acc = 100.00\n","epoch 5/10, step: 410/500.0: loss = 0.00060, acc = 100.00\n","epoch 5/10, step: 420/500.0: loss = 0.00367, acc = 100.00\n","epoch 5/10, step: 430/500.0: loss = 0.00146, acc = 100.00\n","epoch 5/10, step: 440/500.0: loss = 0.00042, acc = 100.00\n","epoch 5/10, step: 450/500.0: loss = 0.00131, acc = 100.00\n","epoch 5/10, step: 460/500.0: loss = 0.00077, acc = 100.00\n","epoch 5/10, step: 470/500.0: loss = 0.00016, acc = 100.00\n","epoch 5/10, step: 480/500.0: loss = 0.00056, acc = 100.00\n","epoch 5/10, step: 490/500.0: loss = 0.00054, acc = 100.00\n","epoch 5/10, step: 500/500.0: loss = 0.00035, acc = 100.00\n","epoch 5/10, val accuracy = 97.54%. Correct 7803 out of 8000 samples\n","epoch 6/10, step: 10/500.0: loss = 0.01543, acc = 98.44\n","epoch 6/10, step: 20/500.0: loss = 0.00135, acc = 100.00\n","epoch 6/10, step: 30/500.0: loss = 0.00272, acc = 100.00\n","epoch 6/10, step: 40/500.0: loss = 0.02769, acc = 98.44\n","epoch 6/10, step: 50/500.0: loss = 0.00898, acc = 100.00\n","epoch 6/10, step: 60/500.0: loss = 0.01708, acc = 98.44\n","epoch 6/10, step: 70/500.0: loss = 0.01112, acc = 98.44\n","epoch 6/10, step: 80/500.0: loss = 0.04700, acc = 98.44\n","epoch 6/10, step: 90/500.0: loss = 0.00138, acc = 100.00\n","epoch 6/10, step: 100/500.0: loss = 0.00151, acc = 100.00\n","epoch 6/10, step: 110/500.0: loss = 0.00867, acc = 100.00\n","epoch 6/10, step: 120/500.0: loss = 0.00002, acc = 100.00\n","epoch 6/10, step: 130/500.0: loss = 0.00019, acc = 100.00\n","epoch 6/10, step: 140/500.0: loss = 0.00463, acc = 100.00\n","epoch 6/10, step: 150/500.0: loss = 0.02601, acc = 98.44\n","epoch 6/10, step: 160/500.0: loss = 0.01186, acc = 98.44\n","epoch 6/10, step: 170/500.0: loss = 0.01333, acc = 100.00\n","epoch 6/10, step: 180/500.0: loss = 0.01238, acc = 98.44\n","epoch 6/10, step: 190/500.0: loss = 0.00017, acc = 100.00\n","epoch 6/10, step: 200/500.0: loss = 0.06734, acc = 96.88\n","epoch 6/10, step: 210/500.0: loss = 0.01846, acc = 98.44\n","epoch 6/10, step: 220/500.0: loss = 0.00353, acc = 100.00\n","epoch 6/10, step: 230/500.0: loss = 0.00821, acc = 100.00\n","epoch 6/10, step: 240/500.0: loss = 0.00189, acc = 100.00\n","epoch 6/10, step: 250/500.0: loss = 0.00172, acc = 100.00\n","epoch 6/10, step: 260/500.0: loss = 0.00496, acc = 100.00\n","epoch 6/10, step: 270/500.0: loss = 0.00022, acc = 100.00\n","epoch 6/10, step: 280/500.0: loss = 0.00541, acc = 100.00\n","epoch 6/10, step: 290/500.0: loss = 0.00321, acc = 100.00\n","epoch 6/10, step: 300/500.0: loss = 0.03377, acc = 98.44\n","epoch 6/10, step: 310/500.0: loss = 0.00017, acc = 100.00\n","epoch 6/10, step: 320/500.0: loss = 0.00093, acc = 100.00\n","epoch 6/10, step: 330/500.0: loss = 0.00141, acc = 100.00\n","epoch 6/10, step: 340/500.0: loss = 0.00749, acc = 100.00\n","epoch 6/10, step: 350/500.0: loss = 0.00067, acc = 100.00\n","epoch 6/10, step: 360/500.0: loss = 0.00481, acc = 100.00\n","epoch 6/10, step: 370/500.0: loss = 0.00106, acc = 100.00\n","epoch 6/10, step: 380/500.0: loss = 0.03876, acc = 98.44\n","epoch 6/10, step: 390/500.0: loss = 0.00651, acc = 100.00\n","epoch 6/10, step: 400/500.0: loss = 0.00033, acc = 100.00\n","epoch 6/10, step: 410/500.0: loss = 0.00040, acc = 100.00\n","epoch 6/10, step: 420/500.0: loss = 0.00033, acc = 100.00\n","epoch 6/10, step: 430/500.0: loss = 0.00012, acc = 100.00\n","epoch 6/10, step: 440/500.0: loss = 0.00026, acc = 100.00\n","epoch 6/10, step: 450/500.0: loss = 0.01039, acc = 100.00\n","epoch 6/10, step: 460/500.0: loss = 0.02429, acc = 98.44\n","epoch 6/10, step: 470/500.0: loss = 0.00042, acc = 100.00\n","epoch 6/10, step: 480/500.0: loss = 0.00736, acc = 100.00\n","epoch 6/10, step: 490/500.0: loss = 0.00058, acc = 100.00\n","epoch 6/10, step: 500/500.0: loss = 0.00024, acc = 100.00\n","epoch 6/10, val accuracy = 99.22%. Correct 7938 out of 8000 samples\n","epoch 7/10, step: 10/500.0: loss = 0.00017, acc = 100.00\n","epoch 7/10, step: 20/500.0: loss = 0.00078, acc = 100.00\n","epoch 7/10, step: 30/500.0: loss = 0.00031, acc = 100.00\n","epoch 7/10, step: 40/500.0: loss = 0.00188, acc = 100.00\n","epoch 7/10, step: 50/500.0: loss = 0.00009, acc = 100.00\n","epoch 7/10, step: 60/500.0: loss = 0.01875, acc = 98.44\n","epoch 7/10, step: 70/500.0: loss = 0.00007, acc = 100.00\n","epoch 7/10, step: 80/500.0: loss = 0.00308, acc = 100.00\n","epoch 7/10, step: 90/500.0: loss = 0.00145, acc = 100.00\n","epoch 7/10, step: 100/500.0: loss = 0.00033, acc = 100.00\n","epoch 7/10, step: 110/500.0: loss = 0.00330, acc = 100.00\n","epoch 7/10, step: 120/500.0: loss = 0.00004, acc = 100.00\n","epoch 7/10, step: 130/500.0: loss = 0.00019, acc = 100.00\n","epoch 7/10, step: 140/500.0: loss = 0.00117, acc = 100.00\n","epoch 7/10, step: 150/500.0: loss = 0.00025, acc = 100.00\n","epoch 7/10, step: 160/500.0: loss = 0.00476, acc = 100.00\n","epoch 7/10, step: 170/500.0: loss = 0.00014, acc = 100.00\n","epoch 7/10, step: 180/500.0: loss = 0.00042, acc = 100.00\n","epoch 7/10, step: 190/500.0: loss = 0.00467, acc = 100.00\n","epoch 7/10, step: 200/500.0: loss = 0.01918, acc = 98.44\n","epoch 7/10, step: 210/500.0: loss = 0.00119, acc = 100.00\n","epoch 7/10, step: 220/500.0: loss = 0.02550, acc = 98.44\n","epoch 7/10, step: 230/500.0: loss = 0.00913, acc = 100.00\n","epoch 7/10, step: 240/500.0: loss = 0.02972, acc = 98.44\n","epoch 7/10, step: 250/500.0: loss = 0.02048, acc = 98.44\n","epoch 7/10, step: 260/500.0: loss = 0.00700, acc = 100.00\n","epoch 7/10, step: 270/500.0: loss = 0.00116, acc = 100.00\n","epoch 7/10, step: 280/500.0: loss = 0.05171, acc = 98.44\n","epoch 7/10, step: 290/500.0: loss = 0.00567, acc = 100.00\n","epoch 7/10, step: 300/500.0: loss = 0.03793, acc = 98.44\n","epoch 7/10, step: 310/500.0: loss = 0.01498, acc = 98.44\n","epoch 7/10, step: 320/500.0: loss = 0.08270, acc = 98.44\n","epoch 7/10, step: 330/500.0: loss = 0.00189, acc = 100.00\n","epoch 7/10, step: 340/500.0: loss = 0.00470, acc = 100.00\n","epoch 7/10, step: 350/500.0: loss = 0.00454, acc = 100.00\n","epoch 7/10, step: 360/500.0: loss = 0.00526, acc = 100.00\n","epoch 7/10, step: 370/500.0: loss = 0.00069, acc = 100.00\n","epoch 7/10, step: 380/500.0: loss = 0.00087, acc = 100.00\n","epoch 7/10, step: 390/500.0: loss = 0.00083, acc = 100.00\n","epoch 7/10, step: 400/500.0: loss = 0.00030, acc = 100.00\n","epoch 7/10, step: 410/500.0: loss = 0.00024, acc = 100.00\n","epoch 7/10, step: 420/500.0: loss = 0.00058, acc = 100.00\n","epoch 7/10, step: 430/500.0: loss = 0.00392, acc = 100.00\n","epoch 7/10, step: 440/500.0: loss = 0.00067, acc = 100.00\n","epoch 7/10, step: 450/500.0: loss = 0.00109, acc = 100.00\n","epoch 7/10, step: 460/500.0: loss = 0.00092, acc = 100.00\n","epoch 7/10, step: 470/500.0: loss = 0.00006, acc = 100.00\n","epoch 7/10, step: 480/500.0: loss = 0.00008, acc = 100.00\n","epoch 7/10, step: 490/500.0: loss = 0.00137, acc = 100.00\n","epoch 7/10, step: 500/500.0: loss = 0.00242, acc = 100.00\n","epoch 7/10, val accuracy = 98.89%. Correct 7911 out of 8000 samples\n","epoch 8/10, step: 10/500.0: loss = 0.00033, acc = 100.00\n","epoch 8/10, step: 20/500.0: loss = 0.02639, acc = 98.44\n","epoch 8/10, step: 30/500.0: loss = 0.00172, acc = 100.00\n","epoch 8/10, step: 40/500.0: loss = 0.01505, acc = 98.44\n","epoch 8/10, step: 50/500.0: loss = 0.00027, acc = 100.00\n","epoch 8/10, step: 60/500.0: loss = 0.00545, acc = 100.00\n","epoch 8/10, step: 70/500.0: loss = 0.00001, acc = 100.00\n","epoch 8/10, step: 80/500.0: loss = 0.00329, acc = 100.00\n","epoch 8/10, step: 90/500.0: loss = 0.01001, acc = 100.00\n","epoch 8/10, step: 100/500.0: loss = 0.01161, acc = 98.44\n","epoch 8/10, step: 110/500.0: loss = 0.00005, acc = 100.00\n","epoch 8/10, step: 120/500.0: loss = 0.00607, acc = 100.00\n","epoch 8/10, step: 130/500.0: loss = 0.00007, acc = 100.00\n","epoch 8/10, step: 140/500.0: loss = 0.00029, acc = 100.00\n","epoch 8/10, step: 150/500.0: loss = 0.00072, acc = 100.00\n","epoch 8/10, step: 160/500.0: loss = 0.00005, acc = 100.00\n","epoch 8/10, step: 170/500.0: loss = 0.00003, acc = 100.00\n","epoch 8/10, step: 180/500.0: loss = 0.00003, acc = 100.00\n","epoch 8/10, step: 190/500.0: loss = 0.00009, acc = 100.00\n","epoch 8/10, step: 200/500.0: loss = 0.00014, acc = 100.00\n","epoch 8/10, step: 210/500.0: loss = 0.00009, acc = 100.00\n","epoch 8/10, step: 220/500.0: loss = 0.00343, acc = 100.00\n","epoch 8/10, step: 230/500.0: loss = 0.00010, acc = 100.00\n","epoch 8/10, step: 240/500.0: loss = 0.02079, acc = 98.44\n","epoch 8/10, step: 250/500.0: loss = 0.00018, acc = 100.00\n","epoch 8/10, step: 260/500.0: loss = 0.00177, acc = 100.00\n","epoch 8/10, step: 270/500.0: loss = 0.00035, acc = 100.00\n","epoch 8/10, step: 280/500.0: loss = 0.00042, acc = 100.00\n","epoch 8/10, step: 290/500.0: loss = 0.00111, acc = 100.00\n","epoch 8/10, step: 300/500.0: loss = 0.00047, acc = 100.00\n","epoch 8/10, step: 310/500.0: loss = 0.00003, acc = 100.00\n","epoch 8/10, step: 320/500.0: loss = 0.00527, acc = 100.00\n","epoch 8/10, step: 330/500.0: loss = 0.00078, acc = 100.00\n","epoch 8/10, step: 340/500.0: loss = 0.00012, acc = 100.00\n","epoch 8/10, step: 350/500.0: loss = 0.00004, acc = 100.00\n","epoch 8/10, step: 360/500.0: loss = 0.06635, acc = 98.44\n","epoch 8/10, step: 370/500.0: loss = 0.00013, acc = 100.00\n","epoch 8/10, step: 380/500.0: loss = 0.00702, acc = 100.00\n","epoch 8/10, step: 390/500.0: loss = 0.00168, acc = 100.00\n","epoch 8/10, step: 400/500.0: loss = 0.00194, acc = 100.00\n","epoch 8/10, step: 410/500.0: loss = 0.00493, acc = 100.00\n","epoch 8/10, step: 420/500.0: loss = 0.00035, acc = 100.00\n","epoch 8/10, step: 430/500.0: loss = 0.00019, acc = 100.00\n","epoch 8/10, step: 440/500.0: loss = 0.01118, acc = 100.00\n","epoch 8/10, step: 450/500.0: loss = 0.00054, acc = 100.00\n","epoch 8/10, step: 460/500.0: loss = 0.00052, acc = 100.00\n","epoch 8/10, step: 470/500.0: loss = 0.02014, acc = 98.44\n","epoch 8/10, step: 480/500.0: loss = 0.00093, acc = 100.00\n","epoch 8/10, step: 490/500.0: loss = 0.00033, acc = 100.00\n","epoch 8/10, step: 500/500.0: loss = 0.00106, acc = 100.00\n","epoch 8/10, val accuracy = 98.76%. Correct 7901 out of 8000 samples\n","epoch 9/10, step: 10/500.0: loss = 0.00174, acc = 100.00\n","epoch 9/10, step: 20/500.0: loss = 0.00122, acc = 100.00\n","epoch 9/10, step: 30/500.0: loss = 0.00095, acc = 100.00\n","epoch 9/10, step: 40/500.0: loss = 0.00141, acc = 100.00\n","epoch 9/10, step: 50/500.0: loss = 0.00632, acc = 100.00\n","epoch 9/10, step: 60/500.0: loss = 0.00038, acc = 100.00\n","epoch 9/10, step: 70/500.0: loss = 0.00010, acc = 100.00\n","epoch 9/10, step: 80/500.0: loss = 0.00995, acc = 100.00\n","epoch 9/10, step: 90/500.0: loss = 0.04458, acc = 98.44\n","epoch 9/10, step: 100/500.0: loss = 0.00451, acc = 100.00\n","epoch 9/10, step: 110/500.0: loss = 0.00035, acc = 100.00\n","epoch 9/10, step: 120/500.0: loss = 0.00245, acc = 100.00\n","epoch 9/10, step: 130/500.0: loss = 0.00115, acc = 100.00\n","epoch 9/10, step: 140/500.0: loss = 0.01213, acc = 100.00\n","epoch 9/10, step: 150/500.0: loss = 0.00941, acc = 100.00\n","epoch 9/10, step: 160/500.0: loss = 0.01985, acc = 98.44\n","epoch 9/10, step: 170/500.0: loss = 0.00139, acc = 100.00\n","epoch 9/10, step: 180/500.0: loss = 0.00330, acc = 100.00\n","epoch 9/10, step: 190/500.0: loss = 0.00739, acc = 100.00\n","epoch 9/10, step: 200/500.0: loss = 0.00603, acc = 100.00\n","epoch 9/10, step: 210/500.0: loss = 0.00384, acc = 100.00\n","epoch 9/10, step: 220/500.0: loss = 0.06497, acc = 98.44\n","epoch 9/10, step: 230/500.0: loss = 0.01960, acc = 98.44\n","epoch 9/10, step: 240/500.0: loss = 0.00284, acc = 100.00\n","epoch 9/10, step: 250/500.0: loss = 0.00210, acc = 100.00\n","epoch 9/10, step: 260/500.0: loss = 0.00163, acc = 100.00\n","epoch 9/10, step: 270/500.0: loss = 0.00009, acc = 100.00\n","epoch 9/10, step: 280/500.0: loss = 0.01664, acc = 98.44\n","epoch 9/10, step: 290/500.0: loss = 0.00189, acc = 100.00\n","epoch 9/10, step: 300/500.0: loss = 0.00296, acc = 100.00\n","epoch 9/10, step: 310/500.0: loss = 0.00624, acc = 100.00\n","epoch 9/10, step: 320/500.0: loss = 0.00076, acc = 100.00\n","epoch 9/10, step: 330/500.0: loss = 0.00089, acc = 100.00\n","epoch 9/10, step: 340/500.0: loss = 0.00036, acc = 100.00\n","epoch 9/10, step: 350/500.0: loss = 0.00004, acc = 100.00\n","epoch 9/10, step: 360/500.0: loss = 0.00234, acc = 100.00\n","epoch 9/10, step: 370/500.0: loss = 0.00042, acc = 100.00\n","epoch 9/10, step: 380/500.0: loss = 0.00017, acc = 100.00\n","epoch 9/10, step: 390/500.0: loss = 0.00027, acc = 100.00\n","epoch 9/10, step: 400/500.0: loss = 0.00034, acc = 100.00\n","epoch 9/10, step: 410/500.0: loss = 0.01002, acc = 100.00\n","epoch 9/10, step: 420/500.0: loss = 0.02525, acc = 98.44\n","epoch 9/10, step: 430/500.0: loss = 0.00075, acc = 100.00\n","epoch 9/10, step: 440/500.0: loss = 0.01911, acc = 98.44\n","epoch 9/10, step: 450/500.0: loss = 0.00164, acc = 100.00\n","epoch 9/10, step: 460/500.0: loss = 0.00050, acc = 100.00\n","epoch 9/10, step: 470/500.0: loss = 0.00204, acc = 100.00\n","epoch 9/10, step: 480/500.0: loss = 0.00020, acc = 100.00\n","epoch 9/10, step: 490/500.0: loss = 0.00017, acc = 100.00\n","epoch 9/10, step: 500/500.0: loss = 0.00064, acc = 100.00\n","epoch 9/10, val accuracy = 99.08%. Correct 7926 out of 8000 samples\n","epoch 10/10, step: 10/500.0: loss = 0.01185, acc = 98.44\n","epoch 10/10, step: 20/500.0: loss = 0.00039, acc = 100.00\n","epoch 10/10, step: 30/500.0: loss = 0.00296, acc = 100.00\n","epoch 10/10, step: 40/500.0: loss = 0.00114, acc = 100.00\n","epoch 10/10, step: 50/500.0: loss = 0.00008, acc = 100.00\n","epoch 10/10, step: 60/500.0: loss = 0.00008, acc = 100.00\n","epoch 10/10, step: 70/500.0: loss = 0.00005, acc = 100.00\n","epoch 10/10, step: 80/500.0: loss = 0.00173, acc = 100.00\n","epoch 10/10, step: 90/500.0: loss = 0.00033, acc = 100.00\n","epoch 10/10, step: 100/500.0: loss = 0.00008, acc = 100.00\n","epoch 10/10, step: 110/500.0: loss = 0.00090, acc = 100.00\n","epoch 10/10, step: 120/500.0: loss = 0.00025, acc = 100.00\n","epoch 10/10, step: 130/500.0: loss = 0.00006, acc = 100.00\n","epoch 10/10, step: 140/500.0: loss = 0.00759, acc = 100.00\n","epoch 10/10, step: 150/500.0: loss = 0.00482, acc = 100.00\n","epoch 10/10, step: 160/500.0: loss = 0.00030, acc = 100.00\n","epoch 10/10, step: 170/500.0: loss = 0.00073, acc = 100.00\n","epoch 10/10, step: 180/500.0: loss = 0.01171, acc = 100.00\n","epoch 10/10, step: 190/500.0: loss = 0.00072, acc = 100.00\n","epoch 10/10, step: 200/500.0: loss = 0.00110, acc = 100.00\n","epoch 10/10, step: 210/500.0: loss = 0.00155, acc = 100.00\n","epoch 10/10, step: 220/500.0: loss = 0.00018, acc = 100.00\n","epoch 10/10, step: 230/500.0: loss = 0.00018, acc = 100.00\n","epoch 10/10, step: 240/500.0: loss = 0.00007, acc = 100.00\n","epoch 10/10, step: 250/500.0: loss = 0.00019, acc = 100.00\n","epoch 10/10, step: 260/500.0: loss = 0.00040, acc = 100.00\n","epoch 10/10, step: 270/500.0: loss = 0.00003, acc = 100.00\n","epoch 10/10, step: 280/500.0: loss = 0.00005, acc = 100.00\n","epoch 10/10, step: 290/500.0: loss = 0.00210, acc = 100.00\n","epoch 10/10, step: 300/500.0: loss = 0.00496, acc = 100.00\n","epoch 10/10, step: 310/500.0: loss = 0.00026, acc = 100.00\n","epoch 10/10, step: 320/500.0: loss = 0.02059, acc = 98.44\n","epoch 10/10, step: 330/500.0: loss = 0.00359, acc = 100.00\n","epoch 10/10, step: 340/500.0: loss = 0.07306, acc = 98.44\n","epoch 10/10, step: 350/500.0: loss = 0.00007, acc = 100.00\n","epoch 10/10, step: 360/500.0: loss = 0.00254, acc = 100.00\n","epoch 10/10, step: 370/500.0: loss = 0.00310, acc = 100.00\n","epoch 10/10, step: 380/500.0: loss = 0.00205, acc = 100.00\n","epoch 10/10, step: 390/500.0: loss = 0.00022, acc = 100.00\n","epoch 10/10, step: 400/500.0: loss = 0.00673, acc = 100.00\n","epoch 10/10, step: 410/500.0: loss = 0.00060, acc = 100.00\n","epoch 10/10, step: 420/500.0: loss = 0.01433, acc = 98.44\n","epoch 10/10, step: 430/500.0: loss = 0.00061, acc = 100.00\n","epoch 10/10, step: 440/500.0: loss = 0.00058, acc = 100.00\n","epoch 10/10, step: 450/500.0: loss = 0.00356, acc = 100.00\n","epoch 10/10, step: 460/500.0: loss = 0.00016, acc = 100.00\n","epoch 10/10, step: 470/500.0: loss = 0.00140, acc = 100.00\n","epoch 10/10, step: 480/500.0: loss = 0.00051, acc = 100.00\n","epoch 10/10, step: 490/500.0: loss = 0.00003, acc = 100.00\n","epoch 10/10, step: 500/500.0: loss = 0.00322, acc = 100.00\n","epoch 10/10, val accuracy = 99.48%. Correct 7958 out of 8000 samples\n"]}],"source":["\n","# torch.cuda.empty_cache()\n","\n","for epoch in range(num_epoch):\n","    epoch_avg_loss = 0\n","    for i, (imgs, labels) in enumerate(train_loader):\n","        imgs = transform_train(imgs.to(device))\n","        labels = labels.to(device)\n","\n","        labels_hat = model(imgs)\n","        n_corrects = (labels_hat.argmax(axis=1) == labels).sum().item()\n","        loss_value = criterion(labels_hat, labels)\n","        loss_value.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        epoch_avg_loss += loss_value.item()\n","\n","        accuracy = (n_corrects / labels.size(0)) * 100\n","\n","        if (i+1) % 10 == 0:\n","            print(f'epoch {epoch + 1}/{num_epoch}, step: {i + 1}/{train_size/BATCH_SIZE}: loss = {loss_value:.5f}, acc = {accuracy:.2f}')\n","\n","    epoch_loss_history.append(epoch_avg_loss / len(train_loader))\n","\n","    with torch.no_grad():\n","        correct = 0\n","        samples = 0\n","\n","        for i, (images, labels) in enumerate(test_loader):\n","            images = transform_test(images.to(device))\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predictions = outputs.max(1)\n","            correct += (predictions == labels).sum()\n","            samples += predictions.size(0)\n","\n","        accuracy = float(correct) / float(samples) * 100\n","        epoch_acc_history.append(accuracy)\n","\n","        print(f\"epoch {epoch + 1}/{num_epoch}, val accuracy = {accuracy:.2f}%. Correct {correct} out of {samples} samples\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"32nueb0q7t3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130802038,"user_tz":-120,"elapsed":7,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"8d94211c-7154-4a8a-a2e6-a6baaedfca7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.0842718128436245, 0.018885296351567375, 0.012246966485166922, 0.007575517110584769, 0.009055676268711977, 0.008477686364880355, 0.007347928607778158, 0.007166051830171455, 0.008048774138573208, 0.006232338538095064]\n"]}],"source":["print(epoch_loss_history)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"CBXssxex7t3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708130802038,"user_tz":-120,"elapsed":2,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}},"outputId":"cd247c17-fae7-40e7-9fd4-778cda9c3822"},"outputs":[{"output_type":"stream","name":"stdout","text":["[98.46249999999999, 98.85000000000001, 98.7875, 99.38749999999999, 97.5375, 99.225, 98.88749999999999, 98.7625, 99.075, 99.47500000000001]\n"]}],"source":["print(epoch_acc_history)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"nx8FHBd37t3r","executionInfo":{"status":"ok","timestamp":1708130802491,"user_tz":-120,"elapsed":455,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["with open('/content/drive/MyDrive/res/history/resnet_hurricane_W4_xy_s1_loss_history.txt', \"w\") as f:\n","    for item in epoch_loss_history:\n","        f.write(f'{item:.5f}\\n')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"zAo7wYLD7t3r","executionInfo":{"status":"ok","timestamp":1708130802492,"user_tz":-120,"elapsed":4,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["with open('/content/drive/MyDrive/res/history/resnet_hurricane_W4_xy_s1_acc_history.txt', \"w\") as f:\n","    for item in epoch_acc_history:\n","        f.write(f'{item:.5f}\\n')"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"o0Lidr-67t3r","executionInfo":{"status":"ok","timestamp":1708130802492,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["# import pandas as pd\n","\n","# test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n","# total_corrects =  0\n","\n","# classes = set()\n","\n","# for imgs, labels in test_loader:\n","#         for i in range(len(labels)):\n","#                 classes.add(labels[i])\n","\n","# heatmap = pd.DataFrame(data=0, index=list(classes), columns=list(classes))\n","\n","# with torch.no_grad():\n","#         for imgs, labels in test_loader:\n","#                 imgs = imgs.to(device)\n","#                 labels = labels.to(device)\n","\n","#                 output = model(imgs)\n","#                 _, predicted = torch.max(output, 1)\n","\n","\n","#                 for i in range(labels.size(0)):\n","#                         if labels[i].item() == predicted[i].item():\n","#                                 total_corrects += 1\n","\n","#                         heatmap.iloc[labels[i].item(), predicted[i].item()] += 1\n","\n","# _, ax = plt.subplots(figsize=(10, 8))\n","# ax = sns.heatmap(heatmap, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n","# plt.show()"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"adozxn5Z7t3r","executionInfo":{"status":"ok","timestamp":1708130802492,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":["torch.save(model, '/content/drive/MyDrive/res/resnet_hurricane_W4_xy_s1.pth')"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"0PBsZGl77t3r","executionInfo":{"status":"ok","timestamp":1708130802492,"user_tz":-120,"elapsed":3,"user":{"displayName":"Yevhenii Trochun","userId":"17711385059759821998"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"294ee4d107f05ff1d3e2c0a05728a96d0532b46a53afc266da718b2bb9e6a9d0"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}